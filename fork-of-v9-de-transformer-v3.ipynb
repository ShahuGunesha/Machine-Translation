{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "282bff4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:03:53.837553Z",
     "iopub.status.busy": "2025-07-27T08:03:53.837261Z",
     "iopub.status.idle": "2025-07-27T08:03:54.619589Z",
     "shell.execute_reply": "2025-07-27T08:03:54.618516Z"
    },
    "papermill": {
     "duration": 0.790228,
     "end_time": "2025-07-27T08:03:54.621103",
     "exception": false,
     "start_time": "2025-07-27T08:03:53.830875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/validation/val_data1.json\n",
      "/kaggle/input/trainnn/train_data1.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "592f819e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:03:54.632102Z",
     "iopub.status.busy": "2025-07-27T08:03:54.631748Z",
     "iopub.status.idle": "2025-07-27T08:04:04.289563Z",
     "shell.execute_reply": "2025-07-27T08:04:04.288771Z"
    },
    "papermill": {
     "duration": 9.6644,
     "end_time": "2025-07-27T08:04:04.290755",
     "exception": false,
     "start_time": "2025-07-27T08:03:54.626355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\r\n",
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96179e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:04.301946Z",
     "iopub.status.busy": "2025-07-27T08:04:04.301578Z",
     "iopub.status.idle": "2025-07-27T08:04:05.522813Z",
     "shell.execute_reply": "2025-07-27T08:04:05.522077Z"
    },
    "papermill": {
     "duration": 1.228459,
     "end_time": "2025-07-27T08:04:05.524357",
     "exception": false,
     "start_time": "2025-07-27T08:04:04.295898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/trainnn/train_data1.json', 'r') as file: # Replace this path with the dataset path in your local machine\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa544100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:05.535483Z",
     "iopub.status.busy": "2025-07-27T08:04:05.535202Z",
     "iopub.status.idle": "2025-07-27T08:04:05.538637Z",
     "shell.execute_reply": "2025-07-27T08:04:05.537932Z"
    },
    "papermill": {
     "duration": 0.01011,
     "end_time": "2025-07-27T08:04:05.539897",
     "exception": false,
     "start_time": "2025-07-27T08:04:05.529787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process JSON data\n",
    "source_sentences_train = []\n",
    "target_sentences_train = []\n",
    "\n",
    "source_sentences_val = []\n",
    "target_sentences_val = []\n",
    "\n",
    "id_train = []\n",
    "id_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ac5a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:05.550622Z",
     "iopub.status.busy": "2025-07-27T08:04:05.550354Z",
     "iopub.status.idle": "2025-07-27T08:04:05.554437Z",
     "shell.execute_reply": "2025-07-27T08:04:05.553641Z"
    },
    "papermill": {
     "duration": 0.010873,
     "end_time": "2025-07-27T08:04:05.555705",
     "exception": false,
     "start_time": "2025-07-27T08:04:05.544832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Pair: English-Bengali\n",
      "Language Pair: English-Hindi\n"
     ]
    }
   ],
   "source": [
    "for language_pair, language_data in data.items():\n",
    "  print(f\"Language Pair: {language_pair}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b03b8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:05.566284Z",
     "iopub.status.busy": "2025-07-27T08:04:05.566081Z",
     "iopub.status.idle": "2025-07-27T08:04:05.609056Z",
     "shell.execute_reply": "2025-07-27T08:04:05.608312Z"
    },
    "papermill": {
     "duration": 0.049413,
     "end_time": "2025-07-27T08:04:05.610198",
     "exception": false,
     "start_time": "2025-07-27T08:04:05.560785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Pair: English-Hindi\n",
      "  Data Type: Train\n"
     ]
    }
   ],
   "source": [
    "for language_pair, language_data in data.items():\n",
    "    if(language_pair == \"English-Hindi\"):\n",
    "      print(f\"Language Pair: {language_pair}\")\n",
    "      for data_type, data_entries in language_data.items():\n",
    "          print(f\"  Data Type: {data_type}\")\n",
    "          for entry_id, entry_data in data_entries.items():\n",
    "              source = entry_data[\"source\"]\n",
    "              target = entry_data[\"target\"]\n",
    "              if (data_type == \"Validation\"):\n",
    "                source_sentences_val.append(source)\n",
    "                target_sentences_val.append(target)\n",
    "                id_val.append(entry_id)\n",
    "              else:\n",
    "                source_sentences_train.append(source)\n",
    "                target_sentences_train.append(target)\n",
    "                id_train.append(entry_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5270e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:05.621051Z",
     "iopub.status.busy": "2025-07-27T08:04:05.620853Z",
     "iopub.status.idle": "2025-07-27T08:04:05.714131Z",
     "shell.execute_reply": "2025-07-27T08:04:05.713364Z"
    },
    "papermill": {
     "duration": 0.100635,
     "end_time": "2025-07-27T08:04:05.715867",
     "exception": false,
     "start_time": "2025-07-27T08:04:05.615232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/validation/val_data1.json', 'r') as file: # Replace this path with the dataset path in your local machine\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d4d43a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:05.728076Z",
     "iopub.status.busy": "2025-07-27T08:04:05.727831Z",
     "iopub.status.idle": "2025-07-27T08:04:05.744210Z",
     "shell.execute_reply": "2025-07-27T08:04:05.743293Z"
    },
    "papermill": {
     "duration": 0.023614,
     "end_time": "2025-07-27T08:04:05.745555",
     "exception": false,
     "start_time": "2025-07-27T08:04:05.721941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Pair: English-Hindi\n",
      "  Data Type: Validation\n"
     ]
    }
   ],
   "source": [
    "for language_pair, language_data in data.items():\n",
    "    if(language_pair == \"English-Hindi\"):\n",
    "      print(f\"Language Pair: {language_pair}\")\n",
    "      for data_type, data_entries in language_data.items():\n",
    "          print(f\"  Data Type: {data_type}\")\n",
    "          for entry_id, entry_data in data_entries.items():\n",
    "              source = entry_data[\"source\"]\n",
    "              #target = entry_data[\"target\"]\n",
    "              if (data_type == \"Validation\"):\n",
    "                source_sentences_val.append(source)\n",
    "                #target_sentences_val.append(target)\n",
    "                #id_val.append(entry_id)\n",
    "              #else:\n",
    "                #source_sentences_train.append(source)\n",
    "                #target_sentences_train.append(target)\n",
    "                #id_train.append(entry_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bd7dee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:05.756889Z",
     "iopub.status.busy": "2025-07-27T08:04:05.756684Z",
     "iopub.status.idle": "2025-07-27T08:04:05.761209Z",
     "shell.execute_reply": "2025-07-27T08:04:05.760452Z"
    },
    "papermill": {
     "duration": 0.011794,
     "end_time": "2025-07-27T08:04:05.762557",
     "exception": false,
     "start_time": "2025-07-27T08:04:05.750763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80797\n",
      "80797\n",
      "11543\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(source_sentences_train))\n",
    "print(len(target_sentences_train))\n",
    "\n",
    "print(len(source_sentences_val))\n",
    "print(len(target_sentences_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4881a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:05.773307Z",
     "iopub.status.busy": "2025-07-27T08:04:05.773090Z",
     "iopub.status.idle": "2025-07-27T08:04:05.821978Z",
     "shell.execute_reply": "2025-07-27T08:04:05.821211Z"
    },
    "papermill": {
     "duration": 0.055607,
     "end_time": "2025-07-27T08:04:05.823215",
     "exception": false,
     "start_time": "2025-07-27T08:04:05.767608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancel everything on my calendar</td>\n",
       "      <td>मेरे कैलेंडर पर सब कुछ रद्द करें</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrenal hormone levels are at their peak durin...</td>\n",
       "      <td>अधिवृक्क के हार्मोन का स्तर प्रातःकाल में अपने...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Golden threads are obtained from Surat, the qu...</td>\n",
       "      <td>स्वर्ण धागे सूरत से प्राप्त होते हैं, जिनकी गु...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Look for agglutination within 30 seconds.</td>\n",
       "      <td>30 सेकेण्ड के भीतर एग्लूटिनेशन देखें।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The non-pompousness and informality of their l...</td>\n",
       "      <td>उनके जीवन की आडंबरहीनता एवं अनौपचारिकता उनके स...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80792</th>\n",
       "      <td>So, is it that this is the optimization proble...</td>\n",
       "      <td>तो, यह अनुकूलन समस्या है जिसमें हम रुचि रखते थे।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80793</th>\n",
       "      <td>In this Masjid made with red stones there are ...</td>\n",
       "      <td>लाल पत्थरों से बनायी गयी इस मस्जिद में हिन्दू ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80794</th>\n",
       "      <td>He began to work on the movie on August 17, 20...</td>\n",
       "      <td>उन्होंने 17 अगस्त, 2010 को फिल्म पर काम करना श...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80795</th>\n",
       "      <td>start a new shopping list</td>\n",
       "      <td>एक नई खरीदारी सूची शुरू करें</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80796</th>\n",
       "      <td>turn off the lights in the kitchen</td>\n",
       "      <td>रसोईघर की बत्तियाँ बंद करें</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80797 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 English  \\\n",
       "0                       cancel everything on my calendar   \n",
       "1      Adrenal hormone levels are at their peak durin...   \n",
       "2      Golden threads are obtained from Surat, the qu...   \n",
       "3              Look for agglutination within 30 seconds.   \n",
       "4      The non-pompousness and informality of their l...   \n",
       "...                                                  ...   \n",
       "80792  So, is it that this is the optimization proble...   \n",
       "80793  In this Masjid made with red stones there are ...   \n",
       "80794  He began to work on the movie on August 17, 20...   \n",
       "80795                          start a new shopping list   \n",
       "80796                 turn off the lights in the kitchen   \n",
       "\n",
       "                                                   Hindi  \n",
       "0                       मेरे कैलेंडर पर सब कुछ रद्द करें  \n",
       "1      अधिवृक्क के हार्मोन का स्तर प्रातःकाल में अपने...  \n",
       "2      स्वर्ण धागे सूरत से प्राप्त होते हैं, जिनकी गु...  \n",
       "3                  30 सेकेण्ड के भीतर एग्लूटिनेशन देखें।  \n",
       "4      उनके जीवन की आडंबरहीनता एवं अनौपचारिकता उनके स...  \n",
       "...                                                  ...  \n",
       "80792   तो, यह अनुकूलन समस्या है जिसमें हम रुचि रखते थे।  \n",
       "80793  लाल पत्थरों से बनायी गयी इस मस्जिद में हिन्दू ...  \n",
       "80794  उन्होंने 17 अगस्त, 2010 को फिल्म पर काम करना श...  \n",
       "80795                       एक नई खरीदारी सूची शुरू करें  \n",
       "80796                        रसोईघर की बत्तियाँ बंद करें  \n",
       "\n",
       "[80797 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x={'English':source_sentences_train,'Hindi':target_sentences_train}\n",
    "df=pd.DataFrame(x)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2611f680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:05.834903Z",
     "iopub.status.busy": "2025-07-27T08:04:05.834693Z",
     "iopub.status.idle": "2025-07-27T08:04:15.671673Z",
     "shell.execute_reply": "2025-07-27T08:04:15.670680Z"
    },
    "papermill": {
     "duration": 9.844334,
     "end_time": "2025-07-27T08:04:15.673149",
     "exception": false,
     "start_time": "2025-07-27T08:04:05.828815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting indic-nlp-library\r\n",
      "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\r\n",
      "Collecting sphinx-argparse (from indic-nlp-library)\r\n",
      "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (0.2.4)\r\n",
      "Collecting morfessor (from indic-nlp-library)\r\n",
      "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.2.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2025.1)\r\n",
      "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (8.1.3)\r\n",
      "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\r\n",
      "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\r\n",
      "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.1)\r\n",
      "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\r\n",
      "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.16.0)\r\n",
      "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\r\n",
      "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\r\n",
      "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\r\n",
      "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\r\n",
      "Requirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->indic-nlp-library) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->indic-nlp-library) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->indic-nlp-library) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->indic-nlp-library) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->indic-nlp-library) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.1.31)\r\n",
      "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\r\n",
      "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\r\n",
      "Installing collected packages: morfessor, sphinx-argparse, indic-nlp-library\r\n",
      "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2\r\n",
      "Cloning into 'indic_nlp_resources'...\r\n",
      "remote: Enumerating objects: 139, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (13/13), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\r\n",
      "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (139/139), 149.77 MiB | 43.49 MiB/s, done.\r\n",
      "Resolving deltas: 100% (53/53), done.\r\n"
     ]
    }
   ],
   "source": [
    "# Install Indic NLP Library\n",
    "!pip install indic-nlp-library\n",
    "\n",
    "# Install Indic NLP Resources (corpus and models)\n",
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
    "import os\n",
    "os.environ[\"INDIC_RESOURCES_PATH\"] = \"/kaggle/working/indic_nlp_resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c5986f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:15.688663Z",
     "iopub.status.busy": "2025-07-27T08:04:15.688367Z",
     "iopub.status.idle": "2025-07-27T08:04:15.702812Z",
     "shell.execute_reply": "2025-07-27T08:04:15.702175Z"
    },
    "papermill": {
     "duration": 0.023197,
     "end_time": "2025-07-27T08:04:15.703942",
     "exception": false,
     "start_time": "2025-07-27T08:04:15.680745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "\n",
    "# Tokenization and Lowercasing\n",
    "def preprocess_english(sentences):\n",
    "    tokenized = []\n",
    "    for sent in sentences:\n",
    "        # Tokenize first, then process tokens\n",
    "        tokens = nltk.word_tokenize(sent.lower())\n",
    "        # Filter punctuation/digits after tokenization\n",
    "        cleaned = [t for t in tokens if t not in string.punctuation and not t.isdigit()]\n",
    "        tokenized.append(cleaned)\n",
    "    return tokenized\n",
    "\n",
    "factory = IndicNormalizerFactory()\n",
    "normalizer = factory.get_normalizer(\"hi\")\n",
    "\n",
    "def preprocess_hindi(sentences):\n",
    "    processed = []\n",
    "    hin_punct = re.compile(r'[!\\\"#\\$%&\\'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}~।॥॰‘’“”…–—]')\n",
    "    \n",
    "    for sent in sentences:\n",
    "        # Normalize variations (e.g., nuqta, vowel signs)\n",
    "        normalized = normalizer.normalize(sent)\n",
    "        cleaned = hin_punct.sub(' ', normalized)\n",
    "        tokens = [t.strip() for t in indic_tokenize.trivial_tokenize(cleaned) if t.strip()]\n",
    "        processed.append(tokens)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# Bengali preprocessing\n",
    "factory_bn = IndicNormalizerFactory()\n",
    "normalizer_bn = factory_bn.get_normalizer(\"bn\")\n",
    "\n",
    "def preprocess_bengali(sentences):\n",
    "    processed = []\n",
    "    bn_punct = re.compile(r'[!\\\"#\\$%&\\'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}~।‘’“”…–—৷৹৺]')\n",
    "    \n",
    "    for sent in sentences:\n",
    "        # Normalize Bengali variants\n",
    "        normalized = normalizer_bn.normalize(sent)\n",
    "        # Remove punctuation\n",
    "        cleaned = bn_punct.sub(' ', normalized)\n",
    "        # Tokenize with IndicNLP\n",
    "        tokens = [t.strip() for t in indic_tokenize.trivial_tokenize(cleaned) if t.strip()]\n",
    "        processed.append(tokens)\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f6648d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:15.719124Z",
     "iopub.status.busy": "2025-07-27T08:04:15.718884Z",
     "iopub.status.idle": "2025-07-27T08:04:15.721923Z",
     "shell.execute_reply": "2025-07-27T08:04:15.721370Z"
    },
    "papermill": {
     "duration": 0.011678,
     "end_time": "2025-07-27T08:04:15.723022",
     "exception": false,
     "start_time": "2025-07-27T08:04:15.711344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check alignment between source and target\n",
    "assert len(source_sentences_train) == len(target_sentences_train), \"Mismatched train data!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b79907b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:15.737817Z",
     "iopub.status.busy": "2025-07-27T08:04:15.737611Z",
     "iopub.status.idle": "2025-07-27T08:04:30.686979Z",
     "shell.execute_reply": "2025-07-27T08:04:30.685841Z"
    },
    "papermill": {
     "duration": 14.958342,
     "end_time": "2025-07-27T08:04:30.688448",
     "exception": false,
     "start_time": "2025-07-27T08:04:15.730106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preprocessed Sentences ===\n",
      "\n",
      "Sample 1:\n",
      "Original English: cancel everything on my calendar\n",
      "Processed English: ['cancel', 'everything', 'on', 'my', 'calendar']\n",
      "Original Hindi: मेरे कैलेंडर पर सब कुछ रद्द करें\n",
      "Processed Hindi: ['मेरे', 'कैलेंडर', 'पर', 'सब', 'कुछ', 'रद्द', 'करें']\n",
      "-----------------------------\n",
      "\n",
      "Sample 2:\n",
      "Original English: Adrenal hormone levels are at their peak during the morning and taper off during the evening, reaching a low level around 3 am An important function of these hormones is to regulate vascular muscle tone and to prevent vasocodilation.\n",
      "Processed English: ['adrenal', 'hormone', 'levels', 'are', 'at', 'their', 'peak', 'during', 'the', 'morning', 'and', 'taper', 'off', 'during', 'the', 'evening', 'reaching', 'a', 'low', 'level', 'around', 'am', 'an', 'important', 'function', 'of', 'these', 'hormones', 'is', 'to', 'regulate', 'vascular', 'muscle', 'tone', 'and', 'to', 'prevent', 'vasocodilation']\n",
      "Original Hindi: अधिवृक्क के हार्मोन का स्तर प्रातःकाल में अपने शिखर पर होता है और लगभग दोपहर ३ बजे एक निम्न स्तर पर पहुंचते हुए, संध्याकाल में धीरे-धीरे कम हो जाता है। इन हार्मोनों का वाहिकीय मांसपेशी के तान को नियंत्रित करने और वाहिकाविस्फार से बचाने का एक महत्त्वपूर्ण\n",
      "Processed Hindi: ['अधिवृक्क', 'के', 'हार्मोन', 'का', 'स्तर', 'प्रातःकाल', 'में', 'अपने', 'शिखर', 'पर', 'होता', 'है', 'और', 'लगभग', 'दोपहर', '३', 'बजे', 'एक', 'निम्न', 'स्तर', 'पर', 'पहुंचते', 'हुए', 'संध्याकाल', 'में', 'धीरे', 'धीरे', 'कम', 'हो', 'जाता', 'है', 'इन', 'हार्मोनों', 'का', 'वाहिकीय', 'मांसपेशी', 'के', 'तान', 'को', 'नियंत्रित', 'करने', 'और', 'वाहिकाविस्फार', 'से', 'बचाने', 'का', 'एक', 'महत्त्वपूर्ण']\n",
      "-----------------------------\n",
      "\n",
      "Sample 3:\n",
      "Original English: Golden threads are obtained from Surat, the quality being 1200 yards (1080 meters) per tola (11.664 grams).\n",
      "Processed English: ['golden', 'threads', 'are', 'obtained', 'from', 'surat', 'the', 'quality', 'being', 'yards', 'meters', 'per', 'tola', '11.664', 'grams']\n",
      "Original Hindi: स्वर्ण धागे सूरत से प्राप्त होते हैं, जिनकी गुणवत्ता 1200 गज़ (1080 मीटर) प्रति तोला (11.664 ग्राम) है।\n",
      "Processed Hindi: ['स्वर्ण', 'धागे', 'सूरत', 'से', 'प्राप्त', 'होते', 'हैं', 'जिनकी', 'गुणवत्ता', '1200', 'गज़', '1080', 'मीटर', 'प्रति', 'तोला', '11', '664', 'ग्राम', 'है']\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "target_sentences_train = [re.sub(r'[a-zA-Z]','',hi) for hi in target_sentences_train] #optional\n",
    "\n",
    "english_tokens = preprocess_english(source_sentences_train)\n",
    "english_test=preprocess_english(source_sentences_val)\n",
    "hindi_tokens = preprocess_hindi(target_sentences_train)\n",
    "hindi_test=preprocess_hindi(target_sentences_val)\n",
    "\n",
    "en_train=english_tokens\n",
    "en_test=english_test\n",
    "de_train=hindi_tokens\n",
    "de_test=hindi_test\n",
    "\n",
    "# Print first 3 preprocessed sentences (English and Hindi)\n",
    "print(\"=== Preprocessed Sentences ===\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(\"Original English:\", source_sentences_train[i])\n",
    "    print(\"Processed English:\", en_train[i])\n",
    "    print(\"Original Hindi:\", target_sentences_train[i])\n",
    "    print(\"Processed Hindi:\", de_train[i])\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ede6e8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:30.704708Z",
     "iopub.status.busy": "2025-07-27T08:04:30.704367Z",
     "iopub.status.idle": "2025-07-27T08:04:31.318572Z",
     "shell.execute_reply": "2025-07-27T08:04:31.317806Z"
    },
    "papermill": {
     "duration": 0.623645,
     "end_time": "2025-07-27T08:04:31.320188",
     "exception": false,
     "start_time": "2025-07-27T08:04:30.696543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tokenized_sentences, max_vocab_size=60000):\n",
    "    # Count all tokens across sentences\n",
    "    word_counts = Counter([word for sent in tokenized_sentences for word in sent])\n",
    "    # Create vocab with special tokens + top-N frequent words\n",
    "    vocab = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"] + [word for word, _ in word_counts.most_common(max_vocab_size)]\n",
    "    return vocab\n",
    "\n",
    "# Build vocabularies for English and Hindi\n",
    "en_index2word = build_vocab(en_train)\n",
    "de_index2word = build_vocab(de_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c067aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:31.336478Z",
     "iopub.status.busy": "2025-07-27T08:04:31.336218Z",
     "iopub.status.idle": "2025-07-27T08:04:31.339691Z",
     "shell.execute_reply": "2025-07-27T08:04:31.338862Z"
    },
    "papermill": {
     "duration": 0.012931,
     "end_time": "2025-07-27T08:04:31.341216",
     "exception": false,
     "start_time": "2025-07-27T08:04:31.328285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<SOS>', '<EOS>', '<UNK>', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(en_index2word[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06443e96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:31.356204Z",
     "iopub.status.busy": "2025-07-27T08:04:31.355984Z",
     "iopub.status.idle": "2025-07-27T08:04:31.414519Z",
     "shell.execute_reply": "2025-07-27T08:04:31.413661Z"
    },
    "papermill": {
     "duration": 0.067339,
     "end_time": "2025-07-27T08:04:31.415733",
     "exception": false,
     "start_time": "2025-07-27T08:04:31.348394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebb55795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:31.431268Z",
     "iopub.status.busy": "2025-07-27T08:04:31.431020Z",
     "iopub.status.idle": "2025-07-27T08:04:31.508327Z",
     "shell.execute_reply": "2025-07-27T08:04:31.507479Z"
    },
    "papermill": {
     "duration": 0.086639,
     "end_time": "2025-07-27T08:04:31.509798",
     "exception": false,
     "start_time": "2025-07-27T08:04:31.423159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_word2index length 60004\n",
      "en_lengths: 16.419297746203448\n",
      "de_lengths: 18.210267707959453\n",
      "Max English length: 255\n",
      "Max Hindi length: 219\n"
     ]
    }
   ],
   "source": [
    "en_word2index = {token: idx for idx, token in enumerate(en_index2word)}\n",
    "de_word2index = {token: idx for idx, token in enumerate(de_index2word)}\n",
    "\n",
    "print(\"en_word2index length\", len(en_word2index))\n",
    "\n",
    "en_lengths = sum([len(sent) for sent in en_train])/len(en_train)\n",
    "de_lengths = sum([len(sent) for sent in de_train])/len(de_train)\n",
    "print(\"en_lengths:\", en_lengths)\n",
    "print(\"de_lengths:\", de_lengths)\n",
    "\n",
    "print(\"Max English length:\", max(len(s) for s in en_train))\n",
    "print(\"Max Hindi length:\", max(len(s) for s in de_train))\n",
    "# Adjust seq_length accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8ea8211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:31.525799Z",
     "iopub.status.busy": "2025-07-27T08:04:31.525575Z",
     "iopub.status.idle": "2025-07-27T08:04:31.530584Z",
     "shell.execute_reply": "2025-07-27T08:04:31.529871Z"
    },
    "papermill": {
     "duration": 0.014471,
     "end_time": "2025-07-27T08:04:31.531733",
     "exception": false,
     "start_time": "2025-07-27T08:04:31.517262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "hidden_size = 256\n",
    "epochs = 39\n",
    "\n",
    "def encode_and_pad(vocab, sent, max_length):\n",
    "    sos = [vocab[\"<SOS>\"]]\n",
    "    eos = [vocab[\"<EOS>\"]]\n",
    "    pad = [vocab[\"<PAD>\"]]\n",
    "    unk = vocab[\"<UNK>\"]\n",
    "\n",
    "    if len(sent) < max_length - 2:\n",
    "        n_pads = max_length - 2 - len(sent)\n",
    "        encoded = [vocab.get(word, unk) for word in sent]  # Use .get() for UNK\n",
    "        return sos + encoded + eos + pad * n_pads\n",
    "    else:\n",
    "        encoded = [vocab.get(word, unk) for word in sent[:max_length-2]]  # Use .get()\n",
    "        return sos + encoded + eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17672ee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:31.547145Z",
     "iopub.status.busy": "2025-07-27T08:04:31.546932Z",
     "iopub.status.idle": "2025-07-27T08:04:32.813494Z",
     "shell.execute_reply": "2025-07-27T08:04:32.812539Z"
    },
    "papermill": {
     "duration": 1.27609,
     "end_time": "2025-07-27T08:04:32.815173",
     "exception": false,
     "start_time": "2025-07-27T08:04:31.539083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_train_encoded = [encode_and_pad(en_word2index, sent, seq_length) for sent in en_train]\n",
    "en_test_encoded = [encode_and_pad(en_word2index, sent, seq_length) for sent in en_test]\n",
    "de_train_encoded = [encode_and_pad(de_word2index, sent, seq_length) for sent in de_train]\n",
    "de_test_encoded = [encode_and_pad(de_word2index, sent, seq_length) for sent in de_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12757096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:32.831329Z",
     "iopub.status.busy": "2025-07-27T08:04:32.831036Z",
     "iopub.status.idle": "2025-07-27T08:04:32.836560Z",
     "shell.execute_reply": "2025-07-27T08:04:32.835859Z"
    },
    "papermill": {
     "duration": 0.01461,
     "end_time": "2025-07-27T08:04:32.837728",
     "exception": false,
     "start_time": "2025-07-27T08:04:32.823118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 8308,\n",
       " 4548,\n",
       " 1432,\n",
       " 13,\n",
       " 25,\n",
       " 60,\n",
       " 1242,\n",
       " 80,\n",
       " 4,\n",
       " 494,\n",
       " 6,\n",
       " 29732,\n",
       " 296,\n",
       " 80,\n",
       " 4,\n",
       " 891,\n",
       " 1385,\n",
       " 10,\n",
       " 504,\n",
       " 233,\n",
       " 169,\n",
       " 143,\n",
       " 36,\n",
       " 132,\n",
       " 216,\n",
       " 5,\n",
       " 49,\n",
       " 5417,\n",
       " 9,\n",
       " 8,\n",
       " 9408,\n",
       " 7505,\n",
       " 3283,\n",
       " 4952,\n",
       " 6,\n",
       " 8,\n",
       " 853,\n",
       " 29733,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train_encoded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6604876d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:32.853557Z",
     "iopub.status.busy": "2025-07-27T08:04:32.853297Z",
     "iopub.status.idle": "2025-07-27T08:04:33.956750Z",
     "shell.execute_reply": "2025-07-27T08:04:33.955764Z"
    },
    "papermill": {
     "duration": 1.112881,
     "end_time": "2025-07-27T08:04:33.958244",
     "exception": false,
     "start_time": "2025-07-27T08:04:32.845363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_x = np.array(en_train_encoded)\n",
    "train_y = np.array(de_train_encoded)\n",
    "test_x = np.array(en_test_encoded)\n",
    "test_y = np.array(de_test_encoded)\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "test_ds = TensorDataset(torch.from_numpy(test_x))\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "#test_dl = DataLoader(test_ds, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "334c3e10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:33.974514Z",
     "iopub.status.busy": "2025-07-27T08:04:33.974222Z",
     "iopub.status.idle": "2025-07-27T08:04:33.979226Z",
     "shell.execute_reply": "2025-07-27T08:04:33.978579Z"
    },
    "papermill": {
     "duration": 0.014538,
     "end_time": "2025-07-27T08:04:33.980574",
     "exception": false,
     "start_time": "2025-07-27T08:04:33.966036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,  8308,  4548,  1432,    13,    25,    60,  1242,    80,\n",
       "           4,   494,     6, 29732,   296,    80,     4,   891,  1385,\n",
       "          10,   504,   233,   169,   143,    36,   132,   216,     5,\n",
       "          49,  5417,     9,     8,  9408,  7505,  3283,  4952,     6,\n",
       "           8,   853, 29733,     2,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a80710c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:33.996408Z",
     "iopub.status.busy": "2025-07-27T08:04:33.996173Z",
     "iopub.status.idle": "2025-07-27T08:04:34.029511Z",
     "shell.execute_reply": "2025-07-27T08:04:34.028839Z"
    },
    "papermill": {
     "duration": 0.042661,
     "end_time": "2025-07-27T08:04:34.030790",
     "exception": false,
     "start_time": "2025-07-27T08:04:33.988129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    1,  8308,  4548,  1432,    13,    25,    60,  1242,    80,     4,\n",
       "           494,     6, 29732,   296,    80,     4,   891,  1385,    10,   504,\n",
       "           233,   169,   143,    36,   132,   216,     5,    49,  5417,     9,\n",
       "             8,  9408,  7505,  3283,  4952,     6,     8,   853, 29733,     2,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([    1,  9753,     4,  4067,    12,   257, 11367,     6,    45,  1626,\n",
       "            15,    40,     5,     9,   138,   988,  3295,   355,    13,  1464,\n",
       "           257,    15,  9754,    73, 23056,     6,   343,   343,    54,    20,\n",
       "            30,     5,    99, 13908,    12, 23057,  8103,     4, 12493,    11,\n",
       "           950,    24,     9, 32285,     8,  1769,    12,    13,  1270,     2,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31607aa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:34.046856Z",
     "iopub.status.busy": "2025-07-27T08:04:34.046650Z",
     "iopub.status.idle": "2025-07-27T08:04:34.054324Z",
     "shell.execute_reply": "2025-07-27T08:04:34.053635Z"
    },
    "papermill": {
     "duration": 0.01716,
     "end_time": "2025-07-27T08:04:34.055586",
     "exception": false,
     "start_time": "2025-07-27T08:04:34.038426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8, num_encoder_layers=6, \n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed_src = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.embed_tgt = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        self.transformer = Transformer(\n",
    "            d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None,\n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        src = self.embed_src(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embed_tgt(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_decoder(tgt)\n",
    "        \n",
    "        output = self.transformer(\n",
    "            src, tgt, \n",
    "            src_mask=src_mask, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        return self.fc_out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99e7664c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:34.071915Z",
     "iopub.status.busy": "2025-07-27T08:04:34.071711Z",
     "iopub.status.idle": "2025-07-27T08:04:38.698087Z",
     "shell.execute_reply": "2025-07-27T08:04:38.697167Z"
    },
    "papermill": {
     "duration": 4.636443,
     "end_time": "2025-07-27T08:04:38.699652",
     "exception": false,
     "start_time": "2025-07-27T08:04:34.063209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    len(en_index2word),\n",
    "    len(de_index2word),\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=1024,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Update training loop\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    return torch.triu(torch.ones((sz, sz), dtype=torch.bool), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5058b6b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T08:04:38.716752Z",
     "iopub.status.busy": "2025-07-27T08:04:38.716312Z",
     "iopub.status.idle": "2025-07-27T11:22:41.112771Z",
     "shell.execute_reply": "2025-07-27T11:22:41.111749Z"
    },
    "papermill": {
     "duration": 11882.418207,
     "end_time": "2025-07-27T11:22:41.126225",
     "exception": false,
     "start_time": "2025-07-27T08:04:38.708018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 2.5382\n",
      "Epoch 1 | Loss: 2.3447\n",
      "Epoch 2 | Loss: 2.2533\n",
      "Epoch 3 | Loss: 2.1812\n",
      "Epoch 4 | Loss: 2.1208\n",
      "Epoch 5 | Loss: 2.0695\n",
      "Epoch 6 | Loss: 2.0258\n",
      "Epoch 7 | Loss: 1.9882\n",
      "Epoch 8 | Loss: 1.9554\n",
      "Epoch 9 | Loss: 1.9275\n",
      "Epoch 10 | Loss: 1.9026\n",
      "Epoch 11 | Loss: 1.8816\n",
      "Epoch 12 | Loss: 1.8631\n",
      "Epoch 13 | Loss: 1.8470\n",
      "Epoch 14 | Loss: 1.8338\n",
      "Epoch 15 | Loss: 1.8216\n",
      "Epoch 16 | Loss: 1.8109\n",
      "Epoch 17 | Loss: 1.8007\n",
      "Epoch 18 | Loss: 1.7920\n",
      "Epoch 19 | Loss: 1.7839\n",
      "Epoch 20 | Loss: 1.7761\n",
      "Epoch 21 | Loss: 1.7694\n",
      "Epoch 22 | Loss: 1.7632\n",
      "Epoch 23 | Loss: 1.7572\n",
      "Epoch 24 | Loss: 1.7519\n",
      "Epoch 25 | Loss: 1.7464\n",
      "Epoch 26 | Loss: 1.7416\n",
      "Epoch 27 | Loss: 1.7366\n",
      "Epoch 28 | Loss: 1.7325\n",
      "Epoch 29 | Loss: 1.7280\n",
      "Epoch 30 | Loss: 1.7243\n",
      "Epoch 31 | Loss: 1.7202\n",
      "Epoch 32 | Loss: 1.7169\n",
      "Epoch 33 | Loss: 1.7134\n",
      "Epoch 34 | Loss: 1.7100\n",
      "Epoch 35 | Loss: 1.7070\n",
      "Epoch 36 | Loss: 1.7037\n",
      "Epoch 37 | Loss: 1.7007\n",
      "Epoch 38 | Loss: 1.6980\n"
     ]
    }
   ],
   "source": [
    "input_length = target_length = seq_length\n",
    "\n",
    "SOS = en_word2index[\"<SOS>\"]\n",
    "EOS = en_word2index[\"<EOS>\"]\n",
    "\n",
    "\n",
    "# Update the training loop \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dl:\n",
    "        src = batch[0].to(device)\n",
    "        tgt = batch[1].to(device)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        \n",
    "        src_mask = generate_square_subsequent_mask(src.size(1)).to(device)\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "        \n",
    "        src_padding_mask = (src == 0).to(device)\n",
    "        tgt_padding_mask = (tgt_input == 0).to(device)\n",
    "        \n",
    "        logits = model(\n",
    "            src, tgt_input,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Compute loss and backpropagate\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch} | Loss: {total_loss/len(train_dl):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24e17000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T11:22:41.147958Z",
     "iopub.status.busy": "2025-07-27T11:22:41.147667Z",
     "iopub.status.idle": "2025-07-27T11:22:41.152712Z",
     "shell.execute_reply": "2025-07-27T11:22:41.151825Z"
    },
    "papermill": {
     "duration": 0.017321,
     "end_time": "2025-07-27T11:22:41.154010",
     "exception": false,
     "start_time": "2025-07-27T11:22:41.136689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "val_ids = [ i for i,_ in data[\"English-Hindi\"][\"Validation\"].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1be8194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T11:22:41.175847Z",
     "iopub.status.busy": "2025-07-27T11:22:41.175568Z",
     "iopub.status.idle": "2025-07-27T16:25:34.503121Z",
     "shell.execute_reply": "2025-07-27T16:25:34.501922Z"
    },
    "papermill": {
     "duration": 18173.339931,
     "end_time": "2025-07-27T16:25:34.504463",
     "exception": false,
     "start_time": "2025-07-27T11:22:41.164532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11543/11543 [5:02:53<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "def generate_translation(model, src, max_length=50, beam_size=5):\n",
    "    model.eval()\n",
    "    src = src.unsqueeze(0).to(device)  # Shape: [1, seq_len]\n",
    "    src_seq_len = src.size(1)\n",
    "    \n",
    "    # Encode source sequence\n",
    "    memory = model.transformer.encoder(\n",
    "        model.pos_encoder(model.embed_src(src) * math.sqrt(model.d_model)),\n",
    "        mask=generate_square_subsequent_mask(src_seq_len).to(device),\n",
    "        src_key_padding_mask=(src == 0).to(device)\n",
    "    )\n",
    "    \n",
    "    # Initialize beam search\n",
    "    candidates = [([de_word2index[\"<SOS>\"]], 0.0)]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        new_candidates = []\n",
    "        for seq, score in candidates:\n",
    "            if seq[-1] == de_word2index[\"<EOS>\"]:\n",
    "                new_candidates.append((seq, score))\n",
    "                continue\n",
    "                \n",
    "            tgt = torch.tensor(seq, device=device).unsqueeze(0)  # Shape: [1, current_seq_len]\n",
    "            output = model.transformer.decoder(\n",
    "                model.pos_decoder(model.embed_tgt(tgt) * math.sqrt(model.d_model)),\n",
    "                memory,\n",
    "                tgt_mask=generate_square_subsequent_mask(tgt.size(1)).to(device),\n",
    "                memory_key_padding_mask=(src == 0).to(device)\n",
    "            )\n",
    "            \n",
    "            logits = model.fc_out(output[:, -1, :])\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            topk_scores, topk_ids = log_probs.topk(beam_size)\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                new_seq = seq + [topk_ids[0][i].item()]\n",
    "                new_score = score + topk_scores[0][i].item()\n",
    "                new_candidates.append((new_seq, new_score))\n",
    "                \n",
    "        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "    \n",
    "    best_seq = candidates[0][0][1:-1] if len(candidates[0][0]) > 2 else []\n",
    "    translated_tokens = [de_index2word[idx] for idx in best_seq]\n",
    "    return ' '.join(translated_tokens)\n",
    "\n",
    "# Validation loop (unchanged)\n",
    "val_outs = []\n",
    "for i in tqdm(range(len(test_ds))):\n",
    "    src_seq = test_ds[i][0].to(device)\n",
    "    translated = generate_translation(model, src_seq)\n",
    "    val_outs.append(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7ecc122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:25:35.602539Z",
     "iopub.status.busy": "2025-07-27T16:25:35.601990Z",
     "iopub.status.idle": "2025-07-27T16:25:35.856032Z",
     "shell.execute_reply": "2025-07-27T16:25:35.855099Z"
    },
    "papermill": {
     "duration": 0.804988,
     "end_time": "2025-07-27T16:25:35.857752",
     "exception": false,
     "start_time": "2025-07-27T16:25:35.052764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>505511</td>\n",
       "      <td>इस ओर किसी तरफ से किसी किसी भी इस ओर किसी तरफ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>505512</td>\n",
       "      <td>अल्कोहल अल्कोहल अल्कोहल अल्कोहल अल्कोहल अल्कोह...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>505513</td>\n",
       "      <td>जौ जौ का उपयोग भी माल्ट उत्पादन के लिए भी किया...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>505514</td>\n",
       "      <td>&lt;UNK&gt; &lt;UNK&gt; ने इस फिल्म के लिए &lt;UNK&gt; &lt;UNK&gt; &lt;UN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>505515</td>\n",
       "      <td>&lt;UNK&gt; &lt;UNK&gt; &lt;UNK&gt; &lt;UNK&gt; नेशनल पार्क वर्ल्ड &lt;UN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>517049</td>\n",
       "      <td>अपने पुत्र भोज के साथ काँच के पुत्र भोज और उनक...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>517050</td>\n",
       "      <td>यह मुख्य रूप से उन क्षेत्रों में होता है जो मु...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>517051</td>\n",
       "      <td>मुझे हिमाचल प्रदेश हिमाचल प्रदेश हिमाचल प्रदेश...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11541</th>\n",
       "      <td>517052</td>\n",
       "      <td>प्राग में लोग चाहेंगे जाने वाले लोगों के मौसम ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11542</th>\n",
       "      <td>517053</td>\n",
       "      <td>मैं कैसे मैं कैसे पकाना आउट कैसे पकाना करता हू...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11543 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                        Translation\n",
       "0      505511  इस ओर किसी तरफ से किसी किसी भी इस ओर किसी तरफ ...\n",
       "1      505512  अल्कोहल अल्कोहल अल्कोहल अल्कोहल अल्कोहल अल्कोह...\n",
       "2      505513  जौ जौ का उपयोग भी माल्ट उत्पादन के लिए भी किया...\n",
       "3      505514  <UNK> <UNK> ने इस फिल्म के लिए <UNK> <UNK> <UN...\n",
       "4      505515  <UNK> <UNK> <UNK> <UNK> नेशनल पार्क वर्ल्ड <UN...\n",
       "...       ...                                                ...\n",
       "11538  517049  अपने पुत्र भोज के साथ काँच के पुत्र भोज और उनक...\n",
       "11539  517050  यह मुख्य रूप से उन क्षेत्रों में होता है जो मु...\n",
       "11540  517051  मुझे हिमाचल प्रदेश हिमाचल प्रदेश हिमाचल प्रदेश...\n",
       "11541  517052  प्राग में लोग चाहेंगे जाने वाले लोगों के मौसम ...\n",
       "11542  517053  मैं कैसे मैं कैसे पकाना आउट कैसे पकाना करता हू...\n",
       "\n",
       "[11543 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = pd.DataFrame()\n",
    "df0[\"ID\"] = val_ids\n",
    "df0[\"Translation\"] = val_outs\n",
    "\n",
    "df0.to_csv('/kaggle/working/answersH.csv', index = False)\n",
    "x=pd.read_csv(\"/kaggle/working/answersH.csv\")\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6748299,
     "sourceId": 10862870,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6748347,
     "sourceId": 10862930,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30108.388637,
   "end_time": "2025-07-27T16:25:39.522623",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-27T08:03:51.133986",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
